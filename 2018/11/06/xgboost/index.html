
<!DOCTYPE html>
<html lang="" class="loading">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>eggTargaryen</title>

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate">
    <meta name="keywords" content="eggTargaryen,"> 
    <meta name="description" content="机器学习：
1、bias与variance的含义？
答案
2、Adaboost、GBDT与XGBoost的区别
深度学习：
1、梯度消失/爆炸原因，以及解决方法。
原因,，，，这个也ok
解决方法：,"> 
    <meta name="author" content="eggTargaryen"> 
    <link rel="alternative" href="atom.xml" title="eggTargaryen" type="application/atom+xml"> 
    <link rel="icon" href="/img/logo.png"> 
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <link rel="stylesheet" href="/css/diaspora.css">
</head>
</html>
<body class="loading">
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">xgboost——原理+使用</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>
    <div class="section">
        <div class="article">
    <div class="main">
        <h1 class="title">xgboost——原理+使用</h1>
        <div class="stuff">
            <span>十一月 06, 2018</span>
            
  <ul class="post-tags-list"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/xgboost/">xgboost</a></li></ul>


        </div>
        <div class="content markdown">
            <h1 id="xgboost是什么？"><a href="#xgboost是什么？" class="headerlink" title="xgboost是什么？"></a>xgboost是什么？</h1><p>XGBoost的全称为：eXtreme Gradient Boosting</p>
<p>xgboost原理：官方文档<a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="noopener">论文</a>和<a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">ppt</a>是个好东西。</p>
<p>所以要想知道xgboost是什么首先要知道gradient boosting是什么，而要想知道gradient boosting 是什么就要先知道boosting是什么。</p>
<h3 id="boosting是什么？"><a href="#boosting是什么？" class="headerlink" title="boosting是什么？"></a>boosting是什么？</h3><p>​    Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造一个预测函数系列,然后以一定的方式将他们组合成一个预测函数。Boosting是一种提高任意给定学习算法准确度的方法。</p>
<p>简单概括一下就是boosting方法是一种解决问题的思想，使用若干若分类算法来构建一个强分类算法。</p>
<p>集成学习方法大致可以分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和随机森林。</p>
<p>boosting算法中比较重要的有Adaboost,GBDT,xgboost</p>
<h3 id="什么是Adaboost？"><a href="#什么是Adaboost？" class="headerlink" title="什么是Adaboost？"></a>什么是Adaboost？</h3><p>AdaBoost是最著名的Boosting族算法。开始时，所有样本的权重相同，训练得到第一个基分类器。从第二轮开始，每轮开始前都先根据上一轮基分类器的分类效果调整每个样本的权重，上一轮分错的样本权重提高，分对的样本权重降低。之后根据新得到样本的权重指导本轮中的基分类器训练，即在考虑样本不同权重的情况下得到本轮错误率最低的基分类器。重复以上步骤直至训练到约定的轮数结束，每一轮训练得到一个基分类器。</p>
<h3 id="什么是GBDT？"><a href="#什么是GBDT？" class="headerlink" title="什么是GBDT？"></a>什么是GBDT？</h3><p>GBDT即梯度提升树，提升方法依然采用的是加法模型与前向分布算法。以决策树为基函数的提升方法称为提升树。对分类问题决策树是二叉分类树，对回归问题决策树是二叉决策树。</p>
<h3 id="GBDT与Adaboost的区别："><a href="#GBDT与Adaboost的区别：" class="headerlink" title="GBDT与Adaboost的区别："></a>GBDT与Adaboost的区别：</h3><p>GBDT与Adaboost最主要的区别在于两者如何识别模型的问题。Adaboost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。GBDT通过负梯度来识别问题，通过计算负梯度来改进模型。</p>
<h3 id="xgboost与GBDT的区别："><a href="#xgboost与GBDT的区别：" class="headerlink" title="xgboost与GBDT的区别："></a>xgboost与GBDT的区别：</h3><p>GBDT算法只利用了一阶的导数信息，xgboost对损失函数做了二阶的泰勒展开，并在目标函数之外加入了正则项对整体求最优解，用以权衡目标函数的下降和模型的复杂程度，避免过拟合。所以不考虑细节方面，两者最大的不同就是目标函数的定义。</p>
<ul>
<li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li>
<li>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</li>
<li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</li>
<li>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</li>
<li><p>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</p>
</li>
<li><p>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</p>
</li>
<li><p>xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
</li>
<li><p>可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p>
</li>
</ul>
<p>具体细节可参考<a href="https://www.cnblogs.com/willnote/p/6801496.html" target="_blank" rel="noopener">cnblogs</a></p>
<p><a href="http://www.52cs.org/?p=429" target="_blank" rel="noopener">这个好像打不开了，写的也不错</a></p>
<p><a href="http://wepon.me/files/gbdt.pdf" target="_blank" rel="noopener">中文ppt</a></p>
<h1 id="xgboost怎么用？"><a href="#xgboost怎么用？" class="headerlink" title="xgboost怎么用？"></a>xgboost怎么用？</h1><p><a href="https://github.com/dmlc/xgboost/tree/master/demo/guide-python" target="_blank" rel="noopener">官方案例</a>是个好东西.</p>
<p>其次，下面就随便看看了。</p>
<h3 id="案例1"><a href="#案例1" class="headerlink" title="案例1:"></a>案例1:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    XGBoost案例之蘑菇是否有毒</span></span><br><span class="line"><span class="string">        任务：根据蘑菇的22个特征判断蘑菇是否有毒</span></span><br><span class="line"><span class="string">        数据介绍：</span></span><br><span class="line"><span class="string">            总样本数：8124</span></span><br><span class="line"><span class="string">                -可食用：4208,51.8%</span></span><br><span class="line"><span class="string">                -有毒：3916,48.2%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                -训练样本：6513</span></span><br><span class="line"><span class="string">                -测试样本：1611</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#导入需要的工具包</span></span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据从文件中读出，并为XGBoost训练准备好</span></span><br><span class="line"></span><br><span class="line">my_workpath = <span class="string">'./data/'</span></span><br><span class="line">dtrain = xgb.DMatrix(my_workpath+<span class="string">'agaricus.txt.train'</span>)</span><br><span class="line">dtest = xgb.DMatrix(my_workpath+<span class="string">'agaricus.txt.test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    该数据为libsvm格式的文本数据，libsvm的文件格式(稀疏特征)</span></span><br><span class="line"><span class="string">    -每一行为一个样本：1 3:1 9:1 19:1 21:1 30:1</span></span><br><span class="line"><span class="string">     * 开头的"1"是样本的标签。3,9位特征索引，1,1为特征的值</span></span><br><span class="line"><span class="string">     * 在两类分类中，用1表示正样本，0表示负样本，也支持用[0,1]表示概率用来做标签，表示正样本的概率</span></span><br><span class="line"><span class="string">    XGBoost加载的数据对象存储在对象Dmatrix中，做了存储效率和运行速度的优化</span></span><br><span class="line"><span class="string">    支持三种数据接口：</span></span><br><span class="line"><span class="string">        * libsvm.txt格式数据文件</span></span><br><span class="line"><span class="string">        * 常规矩阵（numpy 2D array）</span></span><br><span class="line"><span class="string">        * xgboost binary buffer file</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置训练参数</span></span><br><span class="line"><span class="comment"># specify parameters via map</span></span><br><span class="line">param = &#123;</span><br><span class="line">    <span class="string">'max_depth'</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">'eta'</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">'silent'</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    max_depth:树的最大深度，缺省值为6，取值范围：[1,∞]</span></span><br><span class="line"><span class="string">    eta:为了防止过拟合，更新过程用到的收缩步长，eta通过缩减特征的权重使提升计算过程更加保守。缺省值为0.3，取值范围为[0,1]</span></span><br><span class="line"><span class="string">    silent:0表示打印出运行时信息，1表示以缄默方式运行，缺省值为0</span></span><br><span class="line"><span class="string">    objective：定义学习任务以及相应的学习目标，'binary:logistic'表示二分类的逻辑回归问题，输出为概率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="comment"># 设置boosting迭代计算参数</span></span><br><span class="line">num_round = <span class="number">2</span></span><br><span class="line">bst = xgb.train(param,dtrain,num_round)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    与scikit-learn结合</span></span><br><span class="line"><span class="string">    -XGBoost提供一个wrapper类，允许模型可以和scikit-learn框架中的其他分类器或者回归器一样对待</span></span><br><span class="line"><span class="string">    XGBoost中分类器为XGBClassifier-模型在构造时传递</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#bst = xgb.XGBClassifier(max_depth=2,learning_rate=1,n_estimators=num_round,silent=True,objective='binary:logistic')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#预测（训练数据上评估 ）</span></span><br><span class="line"><span class="comment"># 模型训练好后，可以用训练好的模型对进行预测</span></span><br><span class="line"><span class="comment"># XGBoost预测的输出时概率，输出值是样本为第一类的概率--&gt;将其概率值转换为0或1</span></span><br><span class="line"></span><br><span class="line">train_preds = bst.predict(dtrain)</span><br><span class="line">train_predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> train_preds]</span><br><span class="line">y_train = dtrain.get_label()</span><br><span class="line">train_accuracy = accuracy_score(y_train,train_predictions)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Train Accuracy:%.2f%%"</span>%(train_accuracy*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#预测（测试集上预测）</span></span><br><span class="line"></span><br><span class="line">preds = bst.predict(dtest)</span><br><span class="line">predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> preds]</span><br><span class="line">y_test = dtest.get_label()</span><br><span class="line">test_accuracy = accuracy_score(y_test,predictions)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Accuracy:%.2f%%"</span>%(test_accuracy*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型可视化</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    可视化模型中的单课树:调用XGBoost的API plot_tree()/to_graphviz()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">xgb.plot_tree(bst,num_trees=<span class="number">0</span>,rankdir=<span class="string">'LR'</span>)</span><br><span class="line">xgb.plot_importance(bst)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    * 第一个参数为训练好的模型</span></span><br><span class="line"><span class="string">    * 第二个参数为要打印的树的索引（从0开始）</span></span><br><span class="line"><span class="string">    * 第三个参数是打印的格式</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h3 id="案例2"><a href="#案例2" class="headerlink" title="案例2:"></a>案例2:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    XGBoost快速入门-与scikit-learn一起使用</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载LibSVM格式数据模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets  <span class="keyword">import</span> load_svmlight_file</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line">my_workpath = <span class="string">'./data/'</span></span><br><span class="line">X_train,Y_train = load_svmlight_file(my_workpath+<span class="string">'agaricus.txt.train'</span>)</span><br><span class="line">X_test,Y_test = load_svmlight_file(my_workpath+<span class="string">'agaricus.txt.test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> X_train.shape</span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置boosting迭代次数</span></span><br><span class="line">num_round = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">bst = XGBClassifier(max_depth=<span class="number">2</span>,learning_rate=<span class="number">1</span>,n_estimators=num_round,silent=<span class="keyword">True</span>,objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line"></span><br><span class="line">bst.fit(X_train,Y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># XGBoost预测出的是概率，这里蘑菇分类是一个二分类问题，输出值是样本为第一类的概率，我们需要将概率值转换为0或1</span></span><br><span class="line">train_preds = bst.predict(X_train)</span><br><span class="line">train_predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> train_preds]</span><br><span class="line">train_accuracy = accuracy_score(Y_train,train_predictions)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Train Accuracy:%.2f%%"</span>%(train_accuracy*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#预测（测试集上预测）</span></span><br><span class="line"></span><br><span class="line">preds = bst.predict(X_test)</span><br><span class="line">predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> preds]</span><br><span class="line">test_accuracy = accuracy_score(Y_test,predictions)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Accuracy:%.2f%%"</span>%(test_accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<h3 id="案例3"><a href="#案例3" class="headerlink" title="案例3:"></a>案例3:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    前面两个例子在训练集和测试集上都检查了模型的性能</span></span><br><span class="line"><span class="string">    实际场景中测试数据是未知的，如何评估模型？</span></span><br><span class="line"><span class="string">    -答案：校验集</span></span><br><span class="line"><span class="string">    校验集：将训练数据的一部分留出来，不参与模型参数训练</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_svmlight_file</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_workpath = <span class="string">'./data/'</span></span><br><span class="line">X_train, Y_train = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.train'</span>)</span><br><span class="line">X_test, Y_test = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> X_train.shape</span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">训练集测试集分离</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># split data into train and test sets,1/3的训练数据作为校验数据</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">test_size = <span class="number">0.33</span></span><br><span class="line"></span><br><span class="line">X_train_part, X_validate, y_train_part, y_validate = train_test_split(X_train, Y_train, test_size=test_size,</span><br><span class="line">                                                                      random_state=seed)</span><br><span class="line">print(X_train_part.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置boosting迭代次数</span></span><br><span class="line">num_round = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">bst = XGBClassifier(max_depth=<span class="number">2</span>, learning_rate=<span class="number">1</span>, n_estimators=num_round, silent=<span class="keyword">True</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line"></span><br><span class="line">bst.fit(X_train_part, y_train_part)</span><br><span class="line"></span><br><span class="line"><span class="comment"># XGBoost预测出的是概率，这里蘑菇分类是一个二分类问题，输出值是样本为第一类的概率，我们需要将概率值转换为0或1</span></span><br><span class="line"><span class="comment"># 校验集上的性能</span></span><br><span class="line">validare_preds = bst.predict(X_validate)</span><br><span class="line">validare_predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> validare_preds]</span><br><span class="line">validare_accuracy = accuracy_score(y_validate, validare_predictions)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"validare Accuracy:%.2f%%"</span> % (validare_accuracy * <span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集上的性能</span></span><br><span class="line"></span><br><span class="line">train_preds = bst.predict(X_train_part)</span><br><span class="line">train_predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> train_preds]</span><br><span class="line">train_accuracy = accuracy_score(y_train_part, train_predictions)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Train Accuracy:%.2f%%"</span> % (train_accuracy * <span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测（测试集上预测）</span></span><br><span class="line"></span><br><span class="line">preds = bst.predict(X_test)</span><br><span class="line">predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> preds]</span><br><span class="line">test_accuracy = accuracy_score(Y_test, predictions)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Accuracy:%.2f%%"</span> % (test_accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<h3 id="案例4"><a href="#案例4" class="headerlink" title="案例4:"></a>案例4:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    学习曲线</span></span><br><span class="line"><span class="string">        模型预测性能随某个变化的学习参数（如训练样本数目、迭代次数）变化情况</span></span><br><span class="line"><span class="string">        例如XGBoosts的迭代次数（树的数目）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_svmlight_file</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span>  matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">my_workpath = <span class="string">'./data/'</span></span><br><span class="line">X_train, Y_train = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.train'</span>)</span><br><span class="line">X_test, Y_test = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> X_train.shape</span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">训练集测试集分离</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># split data into train and test sets,1/3的训练数据作为校验数据</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">test_size = <span class="number">0.33</span></span><br><span class="line"></span><br><span class="line">X_train_part, X_validate, y_train_part, y_validate = train_test_split(X_train, Y_train, test_size=test_size,</span><br><span class="line">                                                                      random_state=seed)</span><br><span class="line">print(X_train_part.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置boosting迭代次数</span></span><br><span class="line">num_round = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">bst = XGBClassifier(max_depth=<span class="number">2</span>, learning_rate=<span class="number">1</span>, n_estimators=num_round, silent=<span class="keyword">True</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line"></span><br><span class="line">eval_set = [(X_train_part,y_train_part),(X_validate,y_validate)]</span><br><span class="line">bst.fit(X_train_part, y_train_part,eval_metric=[<span class="string">"error"</span>,<span class="string">"logloss"</span>],eval_set=eval_set,verbose=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#显示学习曲线</span></span><br><span class="line"><span class="comment">#retrive performance matrics</span></span><br><span class="line">results = bst.evals_result()</span><br><span class="line">print(results)</span><br><span class="line"></span><br><span class="line">epochs_logloss = len(results[<span class="string">'validation_0'</span>][<span class="string">'logloss'</span>])</span><br><span class="line">epochs_error = len(results[<span class="string">'validation_0'</span>][<span class="string">'error'</span>])</span><br><span class="line">print(epochs_logloss)</span><br><span class="line">print(epochs_error)</span><br><span class="line">x_axis_logloss = range(<span class="number">0</span>,epochs_logloss)</span><br><span class="line">x_axis_error = range(<span class="number">0</span>,epochs_error)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot log loss</span></span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line">ax.plot(x_axis_logloss,results[<span class="string">'validation_0'</span>][<span class="string">'logloss'</span>],label=<span class="string">'Train'</span>)</span><br><span class="line">ax.plot(x_axis_logloss,results[<span class="string">'validation_1'</span>][<span class="string">'logloss'</span>],label=<span class="string">'Test'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.ylabel(<span class="string">'Log Loss'</span>)</span><br><span class="line">plt.title(<span class="string">'XGBoost Log Loss'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot classification error</span></span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line">ax.plot(x_axis_error, results[<span class="string">'validation_0'</span>][<span class="string">'error'</span>], label=<span class="string">'Train'</span>)</span><br><span class="line">ax.plot(x_axis_error, results[<span class="string">'validation_1'</span>][<span class="string">'error'</span>], label=<span class="string">'Test'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.ylabel(<span class="string">'Classification Error'</span>)</span><br><span class="line">plt.title(<span class="string">'XGBoost Classification Error'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># make prediction</span></span><br><span class="line">preds = bst.predict(X_test)</span><br><span class="line">predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> preds]</span><br><span class="line"></span><br><span class="line">test_accuracy = accuracy_score(Y_test, predictions)</span><br><span class="line">print(<span class="string">"Test Accuracy: %.2f%%"</span> % (test_accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<h3 id="案例5"><a href="#案例5" class="headerlink" title="案例5:"></a>案例5:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    Early stop:一种防止训练复杂模型过拟合的方法</span></span><br><span class="line"><span class="string">    -监控模型在校验集上的性能：如果在经过固定次数的迭代，校验集上的性能不再提高时，结束训练过程</span></span><br><span class="line"><span class="string">    -当在测试集上的训练下降而在训练集上的性能还提高时，发生了过拟合</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_svmlight_file</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span>  matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">my_workpath = <span class="string">'./data/'</span></span><br><span class="line">X_train, Y_train = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.train'</span>)</span><br><span class="line">X_test, Y_test = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> X_train.shape</span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">训练集测试集分离</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># split data into train and test sets,1/3的训练数据作为校验数据</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">test_size = <span class="number">0.33</span></span><br><span class="line"></span><br><span class="line">X_train_part, X_validate, y_train_part, y_validate = train_test_split(X_train, Y_train, test_size=test_size,</span><br><span class="line">                                                                      random_state=seed)</span><br><span class="line">print(X_train_part.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置boosting迭代次数</span></span><br><span class="line">num_round = <span class="number">100</span></span><br><span class="line"><span class="comment">#bst = XGBClassifier(param)</span></span><br><span class="line"><span class="comment">#bst = XGBClassifier()</span></span><br><span class="line">bst =XGBClassifier(max_depth=<span class="number">2</span>, learning_rate=<span class="number">0.1</span>, n_estimators=num_round, silent=<span class="keyword">True</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line">eval_set =[(X_validate, y_validate)]</span><br><span class="line">bst.fit(X_train_part, y_train_part, early_stopping_rounds=<span class="number">10</span>, eval_metric=<span class="string">"error"</span>, eval_set=eval_set, verbose=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># retrieve performance metrics</span></span><br><span class="line">results = bst.evals_result()</span><br><span class="line"><span class="comment">#print(results)</span></span><br><span class="line"></span><br><span class="line">epochs = len(results[<span class="string">'validation_0'</span>][<span class="string">'error'</span>])</span><br><span class="line">x_axis = range(<span class="number">0</span>, epochs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot log loss</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(x_axis, results[<span class="string">'validation_0'</span>][<span class="string">'error'</span>], label=<span class="string">'Test'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.ylabel(<span class="string">'Error'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Round'</span>)</span><br><span class="line">plt.title(<span class="string">'XGBoost Early Stop'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="案例6"><a href="#案例6" class="headerlink" title="案例6:"></a>案例6:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    交叉验证：</span></span><br><span class="line"><span class="string">        train_test_split将训练数据的一部分流出来做校验，不参与模型参数训练</span></span><br><span class="line"><span class="string">            -优点：速度快</span></span><br><span class="line"><span class="string">            -缺点：训练数据少，一次校验集的划分会带来随机性</span></span><br><span class="line"><span class="string">                答案：交叉验证（cross-valisation,CV）,但训练时间长</span></span><br><span class="line"><span class="string">            -适合训练数据规模较大的情况（如上百万条记录）</span></span><br><span class="line"><span class="string">            -适合训练慢的机器学习模型******</span></span><br><span class="line"><span class="string">        K-折交叉验证：将训练数据等分为k份（k通常的取值为3,5,10）</span></span><br><span class="line"><span class="string">            -重复k次</span></span><br><span class="line"><span class="string">                *每次流出一份做校验，其余k-1份做训练</span></span><br><span class="line"><span class="string">            -k次校验集上的平均性能视为模型在测试集上性能的估计</span></span><br><span class="line"><span class="string">                * 该估计比train_test_split得到的估计方差更小</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    K-折交叉验证</span></span><br><span class="line"><span class="string">        -重复k次</span></span><br><span class="line"><span class="string">            * 每次留出一份做校验，其余k-1次做训练</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        -k次校验集上的平均性能视为模型在测试集上性能的估计</span></span><br><span class="line"><span class="string">            * k次结果可能得到性能估计的均值和该估计的方差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_svmlight_file</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score <span class="comment">#对给定的参数的单个模型进行评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意：如果每类样本不均衡或者类别数目较多，采用StratifiedKFold，将数据集中每一类样本的数据等分</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span>  matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_workpath = <span class="string">'./data/'</span></span><br><span class="line">X_train, Y_train = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.train'</span>)</span><br><span class="line">X_test, Y_test = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造模型</span></span><br><span class="line"><span class="comment">#设置Boosting迭代计算次数</span></span><br><span class="line">num_round = <span class="number">2</span></span><br><span class="line"><span class="comment">#num_round = rang(1,101)</span></span><br><span class="line"><span class="comment"># param_grid = dict(n_estimators=num_round)</span></span><br><span class="line"><span class="comment">#bst = XGBClassifier(param)</span></span><br><span class="line">bst = XGBClassifier(max_depth=<span class="number">2</span>, learning_rate=<span class="number">1</span>, n_estimators=num_round, silent=<span class="keyword">True</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证--会比较慢</span></span><br><span class="line"><span class="comment"># stratified k-fold cross validation evaluation of xgboost model</span></span><br><span class="line">kfold = KFold(n_splits=<span class="number">10</span>, random_state=<span class="number">7</span>)</span><br><span class="line"><span class="comment">#kfold = StratifiedKFold(n_splits=10,random_state=7)</span></span><br><span class="line">fit_params = &#123;<span class="string">'eval_metric'</span>:<span class="string">"logloss"</span>&#125;</span><br><span class="line">results = cross_val_score(bst, X_train, Y_train, cv=kfold)</span><br><span class="line"><span class="comment">#results = cross_val_score(bst,X_train,Y_train,cv=kfold)</span></span><br><span class="line">print(results)</span><br><span class="line">print(<span class="string">"CV Accuracy:%2f%% (%.2f%%)"</span> %(results.mean()*<span class="number">100</span>,results.std()*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<h3 id="案例7"><a href="#案例7" class="headerlink" title="案例7:"></a>案例7:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    参数调优GridSearcnCV:我们可以根据交叉验证评估结果选择最佳参数模型</span></span><br><span class="line"><span class="string">        -输入待调节参数的范围（grid）,对一组参数对应的模型进行评估，并给出最佳模型及参数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 运行 xgboost安装包中的示例程序</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载LibSVM格式数据模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_svmlight_file</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># read in data，数据在xgboost安装的路径下的demo目录,现在copy到代码目录下的data目录</span></span><br><span class="line">my_workpath = <span class="string">'./data/'</span></span><br><span class="line">X_train,y_train = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.train'</span>)</span><br><span class="line">X_test,y_test = load_svmlight_file(my_workpath + <span class="string">'agaricus.txt.test'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置模型训练参数</span></span><br><span class="line"><span class="comment"># specify parameters via map</span></span><br><span class="line">params = &#123;<span class="string">'max_depth'</span>:<span class="number">2</span>, <span class="string">'eta'</span>:<span class="number">0.1</span>, <span class="string">'silent'</span>:<span class="number">0</span>, <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span> &#125;</span><br><span class="line"><span class="keyword">print</span> params</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造模型</span></span><br><span class="line"><span class="comment">#bst = XGBClassifier(param)</span></span><br><span class="line">bst =XGBClassifier(max_depth=<span class="number">2</span>, learning_rate=<span class="number">0.1</span>, silent=<span class="keyword">True</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉验证</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置boosting迭代参数</span></span><br><span class="line">param_test = &#123;</span><br><span class="line">    <span class="string">'n_estimators'</span>:range(<span class="number">1</span>,<span class="number">51</span>,<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line">clf = GridSearchCV(estimator=bst,param_grid=param_test,scoring=<span class="string">'accuracy'</span>,cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(X_train,y_train)</span><br><span class="line">print(clf.grid_scores_,clf.best_estimator_,clf.best_score_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#make prediction</span></span><br><span class="line">preds = clf.predict(X_test)</span><br><span class="line">predictions = [round(value) <span class="keyword">for</span> value <span class="keyword">in</span> preds]</span><br><span class="line"></span><br><span class="line">test_accuracy = accuracy_score(y_test, predictions)</span><br><span class="line">print(<span class="string">"Test Accuracy of gridsearchcv: %.2f%%"</span> % (test_accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="http://music.163.com/song/media/outer/url?id=17177324">
            </audio>
            
        </div>
        
    <div id="gitalk-container" class="comment link" data-ae="false" data-ci="389843d8e8f4a64fb373" data-cs="4aa2f626551b60a563b935cf7f7c5273b68b28aa" data-r="blogissue" data-o="eggtargaryen" data-a="eggtargaryen" data-d="false">查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>
<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>




</html>